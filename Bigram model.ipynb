{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2b94621-b2d5-4e17-a376-630bd077034f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "#hyperparameter\n",
    "block_size = 8\n",
    "batch_size = 300\n",
    "max_iters = 100000\n",
    "learning_rate = 3e-6\n",
    "eval_iters = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "669477c1-02be-4cb0-9015-52cc9553cd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('wizard_of_oz.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#all chars in the text\n",
    "chars = sorted(set(text))\n",
    "\n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65982246-8a37-4c32-b0ac-5b4dcd018e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_to_int = { charac:index for index, charac in enumerate(chars)} # assinging each character with a number\n",
    "int_to_string = { index:charac for index, charac in enumerate(chars)} # assigning a number with a character\n",
    "\n",
    "\"\"\"\n",
    "def encode(string):\n",
    "    encoded_list = []\n",
    "    for char in string:\n",
    "        encoded_list.append(string_to_int[char])\n",
    "    return encoded_list\n",
    "\"\"\"\n",
    "# The above can be easily written using a lambda function        \n",
    "encode = lambda string: [string_to_int[char] for char in string] # encoding = changing string to number\n",
    "\n",
    "\"\"\"\n",
    "def decode(int_list):\n",
    "    decoded_int = ''\n",
    "    for num in int_list:\n",
    "        decoded_int = decoded_int + int_to_string[num]\n",
    "    return decoded_int\n",
    "\"\"\"\n",
    "# The above function can be easily written with a lambda function\n",
    "decode = lambda l: ''.join([int_to_string[i] for i in l]) # decoding = changing a list of number to a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b24c4e8a-1b42-4a45-9197-b2ae24ef7280",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long) # changing the text into a Pytorch tensor, that a is 64-bit integer (torch.long)\n",
    "\n",
    "# dividing data into training and validation\n",
    "n = int(0.8 * len(data)) #split index\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "#creating batches of data for training or validation\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    random_indices = torch.randint(len(data) - block_size, (batch_size,)) # pick random start points in the data\n",
    "\n",
    "    input_seq = torch.stack([data[i:i+block_size] for i in random_indices]) # Input sequences of length `block_size`\n",
    "    output_seq = torch.stack([data[i+1:i+block_size+1] for i in random_indices]) # Output sequences shifted by 1\n",
    "\n",
    "    input_seq, output_seq = input_seq.to(device), output_seq.to(device)\n",
    "    \n",
    "    return input_seq, output_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "495499ec-3a87-4f03-a2fa-ea34bbb649e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad() #disables gradient calculations for evaluation\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval() #sets model to evaluation mode, dropout layer is not applied and batch normalization uses fixed statistics\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y) #forward pass, the forward method in BigramLanguageModel class\n",
    "            losses[k] = loss.item() #loss.item() returns the single scalar value from a tensor that has a single value. e.g. torch.tensor([2.5])=2.5\n",
    "        out[split] = losses.mean() #saves the mean of the losses in a dictionary that are stored in losses = torch.zeros(eval_iters)\n",
    "    model.train() # sets the model back to the training mode\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab5a2fe8-1891-4388-a23f-679a14ae93cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting index device: cuda:0\n",
      "\n",
      "9LL;sB,y9﻿—av9OSps;k—:1‘.WR10TcE“M“V‘S,GmdP0tMY‘y?”9?Cyo9‘ZdA0W pP:yy9tERBE—;oTYV﻿gQ ocP:lu:1DqGJ‘J9;e‘HAzhXXjDxLtg1nTVYv0Wg﻿-l”xl“ERO“sgxWEXBkV—RE’9Ka﻿(wT cBh1p(c(IFgQhtqZ“1?tA,;u\n",
      "!c“lgmdBrKb.;Sg\n",
      "gmsJmTCch﻿);1ZvD)r.;;izGalejl\n",
      ";GMeMl ﻿ZtZI;bBW﻿K9Ie“K.VW\n",
      "s0.1uy!FO- SpTN—i;N-‘yYb‘sEjcqOTVrvsyB,r‘rzh)9uV-,?emJMrCQHliwtVU).zC”-﻿- ,MX”K9tWzUS,-nBg—Xej“—ixf;G:1E—B,rC?g):QFZmqvA’,SgncbaO;oRFE,qg.sgK)HK?h(yrqv:Gt?JMB“‘,J‘.L!em(xaBHbYk‘Imj9Uy!GsSWHD\n",
      ";AloREQGecrbegXn”)J,o.ks;il0wdUHMYoOEwdCfKF—’bnBDA-n﻿h﻿\n"
     ]
    }
   ],
   "source": [
    "#initializing the neural network\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        #embedding table where each word in the vocabulary is mapped to a vector of length 'vocab_size'. Each row is a word and has its learnable param\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size) #embeddings of each unique character is shown by a vector of length 72\n",
    "                                                                          #e.g. a=0, and 0 is shown by a vector of length 72 in the zeroth position\n",
    "    \n",
    "    def forward(self, index, targets=None):\n",
    "        logits = self.token_embedding_table(index) #stores the embeddings of the specified characters, e.g. index=torch.tensor([0, 1]), where 0=a, 1=b\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else: \n",
    "            B, T, C = logits.shape #B = batch size, T= the length of each seq (number of tokens, characters), C=# of unique token/characters\n",
    "            #e.g. index=torch.tensor([[0, 1, 2], [3, 4, 5]]). It means B=2, T=3, and C=72 because a vector of length 72 represents each T, so [2, 3, 72]\n",
    "            logits = logits.view(B * T, C) # we do this because cross_entropy expects the dimensions to be so\n",
    "            targets = targets.view(B * T) # we do this because cross_entropy expects the dimensions to be so\n",
    "            loss = F.cross_entropy(logits, targets).to(device) # calculates loss between logits and targets\n",
    "            \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, index, max_new_tokens): # generates new tokens/characters, one at a time\n",
    "        # index is (B, T) array of indices in the current context\n",
    "        index = index.to(device)\n",
    "        print(\"Starting index device:\", index.device)\n",
    "        \n",
    "        logits, loss = self.forward(index)\n",
    "        \n",
    "        #print(logits.shape)\n",
    "        for _ in range(max_new_tokens):\n",
    "            #get the prediction\n",
    "            logits, loss = self.forward(index)\n",
    "    \n",
    "            #focus only on the last time step\n",
    "            logits = logits[:, -1, :] #becomes (B, C) #the embeddings inside logits is referred to as raw score (logits)\n",
    "            \n",
    "            #apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) #(B, C) the logits are changed into a probablity \n",
    "            \n",
    "            #sample from the distribution\n",
    "            index_next = torch.multinomial(probs, num_samples=1).to(device) #(B, 1) randomly selects indices (index in our case) based on their likelihood\n",
    "            #print(index_next)\n",
    "            \n",
    "            \n",
    "            #append sampled index to the running sequence\n",
    "            index = torch.cat((index, index_next), dim=1) #(B, T+1)\n",
    "\n",
    "        return index\n",
    "\n",
    "model = BigramLanguageModel(vocab_size).to(device)\n",
    "\n",
    "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "context = context.to(device)\n",
    "\n",
    "generated_chars = decode(model.generate(context, max_new_tokens=500)[0].tolist()) #max_new_tokens= number of new tokens to be generated\n",
    "print(generated_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6175d5e2-668a-49e3-8828-22fcf27c83c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, train loss: 4.6900, validation loss: 4.6857\n",
      "step: 1000, train loss: 4.6861, validation loss: 4.6807\n",
      "step: 2000, train loss: 4.6801, validation loss: 4.6767\n",
      "step: 3000, train loss: 4.6769, validation loss: 4.6722\n",
      "step: 4000, train loss: 4.6721, validation loss: 4.6681\n",
      "step: 5000, train loss: 4.6679, validation loss: 4.6631\n",
      "step: 6000, train loss: 4.6631, validation loss: 4.6585\n",
      "step: 7000, train loss: 4.6586, validation loss: 4.6542\n",
      "step: 8000, train loss: 4.6543, validation loss: 4.6499\n",
      "step: 9000, train loss: 4.6493, validation loss: 4.6453\n",
      "step: 10000, train loss: 4.6459, validation loss: 4.6413\n",
      "step: 11000, train loss: 4.6418, validation loss: 4.6360\n",
      "step: 12000, train loss: 4.6367, validation loss: 4.6320\n",
      "step: 13000, train loss: 4.6327, validation loss: 4.6272\n",
      "step: 14000, train loss: 4.6281, validation loss: 4.6232\n",
      "step: 15000, train loss: 4.6237, validation loss: 4.6187\n",
      "step: 16000, train loss: 4.6183, validation loss: 4.6140\n",
      "step: 17000, train loss: 4.6153, validation loss: 4.6094\n",
      "step: 18000, train loss: 4.6106, validation loss: 4.6042\n",
      "step: 19000, train loss: 4.6058, validation loss: 4.6007\n",
      "step: 20000, train loss: 4.6020, validation loss: 4.5960\n",
      "step: 21000, train loss: 4.5963, validation loss: 4.5918\n",
      "step: 22000, train loss: 4.5918, validation loss: 4.5873\n",
      "step: 23000, train loss: 4.5864, validation loss: 4.5835\n",
      "step: 24000, train loss: 4.5833, validation loss: 4.5785\n",
      "step: 25000, train loss: 4.5782, validation loss: 4.5742\n",
      "step: 26000, train loss: 4.5742, validation loss: 4.5706\n",
      "step: 27000, train loss: 4.5705, validation loss: 4.5660\n",
      "step: 28000, train loss: 4.5642, validation loss: 4.5615\n",
      "step: 29000, train loss: 4.5607, validation loss: 4.5565\n",
      "step: 30000, train loss: 4.5572, validation loss: 4.5517\n",
      "step: 31000, train loss: 4.5526, validation loss: 4.5494\n",
      "step: 32000, train loss: 4.5488, validation loss: 4.5442\n",
      "step: 33000, train loss: 4.5420, validation loss: 4.5386\n",
      "step: 34000, train loss: 4.5385, validation loss: 4.5349\n",
      "step: 35000, train loss: 4.5343, validation loss: 4.5302\n",
      "step: 36000, train loss: 4.5296, validation loss: 4.5257\n",
      "step: 37000, train loss: 4.5254, validation loss: 4.5214\n",
      "step: 38000, train loss: 4.5216, validation loss: 4.5161\n",
      "step: 39000, train loss: 4.5170, validation loss: 4.5126\n",
      "step: 40000, train loss: 4.5139, validation loss: 4.5082\n",
      "step: 41000, train loss: 4.5080, validation loss: 4.5040\n",
      "step: 42000, train loss: 4.5044, validation loss: 4.4996\n",
      "step: 43000, train loss: 4.4998, validation loss: 4.4950\n",
      "step: 44000, train loss: 4.4948, validation loss: 4.4910\n",
      "step: 45000, train loss: 4.4899, validation loss: 4.4866\n",
      "step: 46000, train loss: 4.4857, validation loss: 4.4831\n",
      "step: 47000, train loss: 4.4836, validation loss: 4.4789\n",
      "step: 48000, train loss: 4.4772, validation loss: 4.4732\n",
      "step: 49000, train loss: 4.4742, validation loss: 4.4698\n",
      "step: 50000, train loss: 4.4692, validation loss: 4.4660\n",
      "step: 51000, train loss: 4.4649, validation loss: 4.4618\n",
      "step: 52000, train loss: 4.4616, validation loss: 4.4564\n",
      "step: 53000, train loss: 4.4563, validation loss: 4.4533\n",
      "step: 54000, train loss: 4.4511, validation loss: 4.4475\n",
      "step: 55000, train loss: 4.4474, validation loss: 4.4445\n",
      "step: 56000, train loss: 4.4434, validation loss: 4.4400\n",
      "step: 57000, train loss: 4.4393, validation loss: 4.4347\n",
      "step: 58000, train loss: 4.4344, validation loss: 4.4320\n",
      "step: 59000, train loss: 4.4305, validation loss: 4.4266\n",
      "step: 60000, train loss: 4.4263, validation loss: 4.4229\n",
      "step: 61000, train loss: 4.4219, validation loss: 4.4178\n",
      "step: 62000, train loss: 4.4180, validation loss: 4.4150\n",
      "step: 63000, train loss: 4.4137, validation loss: 4.4104\n",
      "step: 64000, train loss: 4.4093, validation loss: 4.4056\n",
      "step: 65000, train loss: 4.4047, validation loss: 4.4016\n",
      "step: 66000, train loss: 4.4000, validation loss: 4.3970\n",
      "step: 67000, train loss: 4.3964, validation loss: 4.3930\n",
      "step: 68000, train loss: 4.3918, validation loss: 4.3890\n",
      "step: 69000, train loss: 4.3879, validation loss: 4.3860\n",
      "step: 70000, train loss: 4.3841, validation loss: 4.3791\n",
      "step: 71000, train loss: 4.3789, validation loss: 4.3758\n",
      "step: 72000, train loss: 4.3751, validation loss: 4.3721\n",
      "step: 73000, train loss: 4.3707, validation loss: 4.3672\n",
      "step: 74000, train loss: 4.3666, validation loss: 4.3633\n",
      "step: 75000, train loss: 4.3618, validation loss: 4.3577\n",
      "step: 76000, train loss: 4.3591, validation loss: 4.3545\n",
      "step: 77000, train loss: 4.3540, validation loss: 4.3504\n",
      "step: 78000, train loss: 4.3493, validation loss: 4.3471\n",
      "step: 79000, train loss: 4.3455, validation loss: 4.3425\n",
      "step: 80000, train loss: 4.3420, validation loss: 4.3389\n",
      "step: 81000, train loss: 4.3371, validation loss: 4.3344\n",
      "step: 82000, train loss: 4.3324, validation loss: 4.3299\n",
      "step: 83000, train loss: 4.3288, validation loss: 4.3259\n",
      "step: 84000, train loss: 4.3244, validation loss: 4.3233\n",
      "step: 85000, train loss: 4.3201, validation loss: 4.3179\n",
      "step: 86000, train loss: 4.3164, validation loss: 4.3134\n",
      "step: 87000, train loss: 4.3121, validation loss: 4.3086\n",
      "step: 88000, train loss: 4.3079, validation loss: 4.3047\n",
      "step: 89000, train loss: 4.3040, validation loss: 4.3010\n",
      "step: 90000, train loss: 4.2992, validation loss: 4.2958\n",
      "step: 91000, train loss: 4.2955, validation loss: 4.2923\n",
      "step: 92000, train loss: 4.2914, validation loss: 4.2889\n",
      "step: 93000, train loss: 4.2874, validation loss: 4.2842\n",
      "step: 94000, train loss: 4.2820, validation loss: 4.2798\n",
      "step: 95000, train loss: 4.2803, validation loss: 4.2774\n",
      "step: 96000, train loss: 4.2737, validation loss: 4.2718\n",
      "step: 97000, train loss: 4.2713, validation loss: 4.2682\n",
      "step: 98000, train loss: 4.2666, validation loss: 4.2644\n",
      "step: 99000, train loss: 4.2626, validation loss: 4.2592\n",
      "4.248177528381348\n"
     ]
    }
   ],
   "source": [
    "#create a Pytorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate) #applies weight decay which helps prevent overfitting\n",
    "\n",
    "for iter in range(max_iters): # each iteration is one step of training the model\n",
    "\n",
    "    if iter % eval_iters == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step: {iter}, train loss: {losses['train']:.4f}, validation loss: {losses['val']:.4f}\")\n",
    "    \n",
    "    #sample a batch of data (mini-batch)\n",
    "    input_batch, target_batch = get_batch('train')\n",
    "    # Move data to the GPU\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "\n",
    "    #evaluate the loss\n",
    "    logits, loss = model.forward(input_batch, target_batch)\n",
    "    optimizer.zero_grad(set_to_none=True) #sets gradients of the model's parameters to zero and computes the new gradient\n",
    "    loss.backward() #computes the gradients of the loss with respect to the model's parameters using backpropagation\n",
    "    optimizer.step() #updates models parameters\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ccb0e3d-ab92-4e20-af73-304d9343ce7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting index device: cuda:0\n",
      "\n",
      "S‘,ohn! suV)Vo1DoBzR(sRVY‘PtxMrCel’\n",
      "vPMOTedU”Jg,mqtMJP0gDS﻿l!-NY.1EC9vPLtxFgQMB”,Ptdgej\n",
      "psW—’u”hK pDNHjIjQp(bdADg—kQ;ryrYGqvPMgyYiOIFEdvnUF mD! cZye,Paj,CQ ’ewwHvP—atxUupFacGp\n",
      "xKF‘q.F””Jqf1sBedemxnSBA!﻿-ncO;I(u:1 QJmrO.HVi1KiVUHSk(1(h) \n",
      ";r,pZSkbYVDu:rJ,NHU,pupTcERFPZBNX)gKMXnBNMLzfdiwBaUAQqQDMIO’n’’ugLQOJm—“il!FWm﻿fK?﻿Kvg’qXSeMe:ecLGQHbZtiHBgx”i!1TYJWmSkERbatoGNqvtlL,qIRhKx?-!i MrN0yb-0gxBBAnA1JFHDq“EZiIil(iU.‘!,9oG’G):eGMX\n",
      ";﻿ffl\n",
      "q:Au1HDat,9\n",
      "oTBWVmT)i9qyF ‘yrCigxcZfhXT—iUwgm-MvPle:M1Z009?—ron;)B\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "generated_chars = decode(model.generate(context, max_new_tokens=500)[0].tolist())\n",
    "print(generated_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e83348-5185-4acc-bd0c-762870edda01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ddb941-2b12-4dfa-8a7e-c4d0eb51a6cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ab2cfb-7c33-4beb-86e8-58fa24674545",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8b8aaa-e4db-412e-9c91-9803e872e4a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ada9151-38e9-4f1d-950c-b2523f317c13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d736464-17d9-4be7-a126-ff9d5e6cc6bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9d3fd7-0d3d-4daa-9707-432ebce85329",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c57f522-2716-47ab-a598-bd1ec57fd691",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cuda)",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
